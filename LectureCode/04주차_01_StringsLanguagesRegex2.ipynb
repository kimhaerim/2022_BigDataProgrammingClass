{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "734c7a7a",
   "metadata": {},
   "source": [
    "# 4주 1강: String, Regular Expressions, and Languages (2)\n",
    "\n",
    "그 이후 정규표현식 (Regular Expressions) 을 통해서 텍스트를 다루는 법을 공부<br>\n",
    "python의 자연어 처리기인 nltk를 통해서 텍스트 데이터를 다루는 법을 배움\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be645e24",
   "metadata": {},
   "source": [
    "# 정규표현식과 re 모듈\n",
    "\n",
    "조금 더 복잡한 규칙을 가진 텍스트를 처리하는 법 <br>\n",
    "ex) B~D까지의 문자를 모두 A라는 문자로 치환하고 싶을 때 \n",
    "\n",
    "모듈 re : 정규표현식 (Regular expression)를 다룸 <br>\n",
    "    - 기본 모듈이라 설치할 필요 없음 <br>\n",
    "    - re.search, re.match : 문장 안에 정규 표현식과 매칭되는 단어가 있는지 찾아줌<br>\n",
    "    - match 처음부터 일치하는 경우, search 중간부터도 찾아줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfc6992d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 4), match='Life'>\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "txt1 = \"Life is too short, you need python\"\n",
    "txt2 = \"My baby is my Life\"\n",
    "\n",
    "print(re.match('Life', txt1))\n",
    "print(re.match('Life', txt2))\n",
    "print(re.match('life', txt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3deb10bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 4), match='Life'>\n",
      "<re.Match object; span=(14, 18), match='Life'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "txt1 = \"Life is too short, you need python\"\n",
    "txt2 = \"My baby is my Life\"\n",
    "\n",
    "print(re.search('Life', txt1)) \n",
    "print(re.search('Life', txt2)) \n",
    "print(re.search('life', txt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4668ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Life\n",
      "0\n",
      "4\n",
      "(0, 4)\n"
     ]
    }
   ],
   "source": [
    "match = re.search('Life', txt1)\n",
    "# group 함수를 통해서 매칭된 값을 얻음\n",
    "\n",
    "print(match.group())\n",
    "\n",
    "# start, end, span 함수를 통해서 텍스트의 어느 부분에서 매칭이 일어났는지 알 수 있음\n",
    "print(match.start())\n",
    "print(match.end())\n",
    "print(match.span())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7d0ffa",
   "metadata": {},
   "source": [
    "대소문자를 구분없이 찾고 싶을 때 <br>\n",
    "1. life Life를 모두 검색 <br>\n",
    "  | 은 or 연산자와 같음 앞과 뒤의 것 중 하나라도 맞으면 됨 <br>\n",
    " -> \"선택 (selection)\" <br>\n",
    "2. Life의 맨 앞글자를 l혹은 L으로 검색하라고 하는 것 <br>\n",
    "  [] 대괄호로 묶어줌\n",
    " -> \"문자 클래스 chracter class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1831a816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 4), match='Life'>\n",
      "<re.Match object; span=(14, 18), match='life'>\n",
      "Life\n",
      "life\n"
     ]
    }
   ],
   "source": [
    "\n",
    "txt1 = \"Life is too short, you need python\"\n",
    "txt2 = \"My baby is my life\"\n",
    "\n",
    "print(re.search(\"Life|life\", txt1))\n",
    "print(re.search(\"Life|life\", txt2))\n",
    "\n",
    "print(re.search(\"Life|life\", txt1).group())\n",
    "print(re.search(\"Life|life\", txt2).group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "127d6676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 4), match='Life'>\n",
      "<re.Match object; span=(14, 18), match='life'>\n",
      "Life\n",
      "life\n"
     ]
    }
   ],
   "source": [
    "print(re.search(\"[Ll]ife\", txt1))\n",
    "print(re.search(\"[Ll]ife\", txt2))\n",
    "\n",
    "print(re.search(\"[Ll]ife\", txt1).group())\n",
    "print(re.search(\"[Ll]ife\", txt2).group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80bfad10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 4), match='Life'>\n",
      "None\n",
      "None\n",
      "<re.Match object; span=(14, 18), match='life'>\n"
     ]
    }
   ],
   "source": [
    "# ^은 문장의 가장 처음을 의미, $은 문장의 가장 마지막을 의미\n",
    "# 아래와 같이 하면 문장의 맨 처음이 매칭되거나 맨 마지막에 매칭되게 할 수 있음\n",
    "# ^와 $를 위치를 이야기한다는 뜻에서 \"앵커 (anchor)\"라고 함\n",
    "\n",
    "txt1 = \"Life is too short, you need python\"\n",
    "txt2 = \"My baby is my life\"\n",
    "\n",
    "print(re.search(\"^[Ll]ife\", txt1))\n",
    "print(re.search(\"^[Ll]ife\", txt2))\n",
    "\n",
    "print(re.search(\"[Ll]ife$\", txt1))\n",
    "print(re.search(\"[Ll]ife$\", txt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31763d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<re.Match object; span=(16, 24), match=' life   '>\n"
     ]
    }
   ],
   "source": [
    "# 앞 뒤로 공백이 있다면 \\s를 사용\n",
    "txt2 = \"   My baby is my life   \"\n",
    "\n",
    "print(re.search(\"[Ll]ife$\", txt2))\n",
    "\n",
    "# \\s*은 공백문자열을 몇개라도 포함하게 해도 괜찮다는 뜻 \n",
    "# * : 수량자\n",
    "print(re.search(\"\\s*[Ll]ife\\s*$\", txt2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e4b328",
   "metadata": {},
   "source": [
    "match, search : 가장 처음 등장한 한 번만 찾아줌 <br>\n",
    "등장하는 모든 경우를 찾고 싶을 때 -> findall <br>\n",
    "  - 리스트로 리턴해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e29cd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python', 'Python', 'Python']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt3 = \"Python, Python, Python ! it is the best language in the world\"\n",
    "re.findall(\"Python\", txt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bf4591",
   "metadata": {},
   "source": [
    "# 메타문자와 정규표현식 \n",
    "메타 문자 : $, ^, [ ], | <br>\n",
    "특별한 의미를 가지는 문자로 정규표현식을 보다 쉽게 표현할 수 있게 해줌 <br>\n",
    ". : 한 문자를 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "723f9da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<re.Match object; span=(0, 4), match='ABBA'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(re.search('A..A', 'ABA'))\n",
    "print(re.search('A..A', 'ABBA'))\n",
    "print(re.search('A..A', 'ABBBA'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef5c2e9",
   "metadata": {},
   "source": [
    "? 은 0개 혹은 1개 문자를 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f501301e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='A'>\n",
      "<re.Match object; span=(0, 1), match='A'>\n",
      "None\n",
      "<re.Match object; span=(3, 4), match='A'>\n",
      "<re.Match object; span=(1, 3), match='AB'>\n",
      "<re.Match object; span=(1, 3), match='AB'>\n"
     ]
    }
   ],
   "source": [
    "print(re.search('AB?', 'A'))\n",
    "print(re.search('AB?', 'AA'))\n",
    "print(re.search('AB?', 'J-HOP'))\n",
    "print(re.search('AB?', 'X-MAN'))\n",
    "print(re.search('AB?', 'CABBA'))\n",
    "print(re.search('AB?', 'CABBBBBA'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86c258a",
   "metadata": {},
   "source": [
    "\"*\" 은 최하 0개 이상의 일치하는 문자를 의미 (0개도 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "030ef5a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='A'>\n",
      "<re.Match object; span=(0, 1), match='A'>\n",
      "None\n",
      "<re.Match object; span=(3, 4), match='A'>\n",
      "<re.Match object; span=(1, 4), match='ABB'>\n",
      "<re.Match object; span=(1, 7), match='ABBBBB'>\n"
     ]
    }
   ],
   "source": [
    "print(re.search('AB*', 'A'))\n",
    "print(re.search('AB*', 'AA'))\n",
    "print(re.search('AB*', 'J-HOP'))\n",
    "print(re.search('AB*', 'X-MAN'))\n",
    "print(re.search('AB*', 'CABBA'))\n",
    "print(re.search('AB*', 'CABBBBBA'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bd3634",
   "metadata": {},
   "source": [
    "\"+\" 은 최하 1개 이상의 일치하는 문자를 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9470587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AI101', 'AI102']\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "# 멀티라인 텍스트는 세 개의 따옴표를 사용하여 표현\n",
    "text=\"\"\"CS101 Python\n",
    "AI101 AIandsociety\n",
    "AI102 PyTorch\n",
    "CS102 TeamProject\n",
    "\"\"\" \n",
    " \n",
    "s = re.findall('AI\\d+', text) \n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa61c08",
   "metadata": {},
   "source": [
    "# sub 함수\n",
    "정규식에서 매칭된 문자를 대치할 수 있음 <br>\n",
    "앞에 찾은 문자열을 뒤의 문자열로 치환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e85719e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's Melon chart #1 is Brave Girls\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "s = \"Today's Melon chart #1 is IU\"\n",
    "print(re.sub('IU', 'Brave Girls', s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "146adca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My password is * * * * *\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'My password is * * * * *'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'My password is 1 3 5 102 41'\n",
    "print(re.sub('[0-9]+', '*', s))\n",
    "re.sub('\\d+', '*', s)     # 숫자만 찾아서 *으로 바꿈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a1bbbe",
   "metadata": {},
   "source": [
    "# 자연어 처리를 위한 nltk 모듈\n",
    "일반적은 자연어는 규칙을 찾아내기 어려움 <br>\n",
    "문장에서 명사만 찾아내는 것은 어려움 <br>\n",
    "python에는 nltk(Natural Language Tool Kit)이라는 패키지가 존재함 <br>\n",
    "한국어 텍스트를 다루고 싶을 때는 konlpy를 같이 설치"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259f389a",
   "metadata": {},
   "source": [
    "# 토큰화\n",
    "수집한 자연어 데이터는 필요에 맞게 전처리된 상태가 아님 <br>\n",
    "해당 데이터를 사용하고자하는 용도에 따라 3단계일을 진행하게 됨<br>\n",
    "1. 토큰화 : 먼저주어진 코퍼스(corpus)단위에서 토큰(token)이라는 작은 단위로 나누는 작업<br>\n",
    "2. 정제 <br>\n",
    "3. 정규화 <br>\n",
    "토큰의 단위는 상황에 따라 다르지만 단어나 단어 몇개가 모인 구를 사용함 <br>\n",
    "nltk의 토큰화에는 nltk.tokenize를 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72882de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/apple/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big data refers to data sets that are too large or complex to be dealt with by traditional data-processing application software\n",
      "['Big', 'data', 'refers', 'to', 'data', 'sets', 'that', 'are', 'too', 'large', 'or', 'complex', 'to', 'be', 'dealt', 'with', 'by', 'traditional', 'data-processing', 'application', 'software']\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "#  punkt라는 데이터셋이 필수적으로 필요합니다.\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Specify the title of the Wikipedia page\n",
    "wiki = wikipedia.page('Big data').content.split(\".\")[0]\n",
    "print(wiki)\n",
    "print(word_tokenize(wiki))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5a2b5d",
   "metadata": {},
   "source": [
    "word_tokenize()는 TreebankWordTokenizer라는 방식으로 토큰화<br>\n",
    "1980년대 월스트리트 저널의 기사로부터 만들어진 방식<br>\n",
    "이 방식은 스페이스와 구두점(.)을 이용해서 단어를 분해하고, 구두점을 남겨둠 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a48a174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'ca', \"n't\", 'stop']\n"
     ]
    }
   ],
   "source": [
    "string = \"I can't stop\"\n",
    "print(word_tokenize(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "465d8f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'can', \"'\", 't', 'stop']\n"
     ]
    }
   ],
   "source": [
    "# WordPunctTokenizer라는 방식도 있음\n",
    "# 구두점 단위로 단순히 나누어줌\n",
    "from nltk.tokenize import WordPunctTokenizer  \n",
    "print(WordPunctTokenizer().tokenize(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8013f9",
   "metadata": {},
   "source": [
    "# 정제와 정규화\n",
    "토큰화 작업 전, 후에는 텍스트 용도에 맞게 정제(cleaning)하거나 정규화(normalization)하는 일이 발생 <br>\n",
    "정제(cleaning) : 갖고 있는 코퍼스로부터 노이즈 데이터를 제거<br>\n",
    "정규화(normalization) : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만들어줌 <br>\n",
    "\n",
    "정제와 정규화 기법<br> \n",
    "아래의 기법들 이외에도 다양한 기법이 있지만, 분석의 목적에 맞게 적절한 방법을 사용해야 함\n",
    "\n",
    "규칙에 기반한 단어의 통합 <br> \n",
    "  * 동일 단어 매칭 (ex: US, USA) <br> \n",
    "  * 확률적인 방법 (embedding) <br> \n",
    "  * 대소문자 통합 <br> \n",
    "\n",
    "불필요한 단어의 제거 <br> \n",
    "  * 불용어 사전 사용 <br> \n",
    "  * 등장 빈도가 적은 단어 삭제 <br> \n",
    "  * 길이가 매우 짧은 단어를 삭제 <br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f74b418b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was wondering anyone out there could enlighten this car.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "print(shortword.sub('', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c49c24",
   "metadata": {},
   "source": [
    "# 품사 태깅 (Part-of-speech tagging)\n",
    "단순히 공백 등으로 나누는 것 말고 단어의 품사를 태깅할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10d9b36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/apple/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big data refers to data sets that are too large or complex to be dealt with by traditional data-processing application software\n",
      "[('Big', 'NNP'), ('data', 'NN'), ('refers', 'NNS'), ('to', 'TO'), ('data', 'NNS'), ('sets', 'NNS'), ('that', 'WDT'), ('are', 'VBP'), ('too', 'RB'), ('large', 'JJ'), ('or', 'CC'), ('complex', 'JJ'), ('to', 'TO'), ('be', 'VB'), ('dealt', 'VBN'), ('with', 'IN'), ('by', 'IN'), ('traditional', 'JJ'), ('data-processing', 'JJ'), ('application', 'NN'), ('software', 'NN')]\n",
      "[('data', 'NN'), ('application', 'NN'), ('software', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "wiki = wikipedia.page('Big data').content.split(\".\")[0]\n",
    "print(wiki)\n",
    "tokens = word_tokenize(wiki)\n",
    "print(pos_tag(tokens))\n",
    "nouns = [x for x in pos_tag(tokens) if x[1] ==\"NN\"]\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b4fd9b",
   "metadata": {},
   "source": [
    "단순 단어가 아닌 \"명사구\" 등의 뭉치를 찾고 싶다면 정규식을 이용한 chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffd7100e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Big/NNP\n",
      "  (NP data/NN)\n",
      "  refers/NNS\n",
      "  to/TO\n",
      "  data/NNS\n",
      "  sets/NNS\n",
      "  that/WDT\n",
      "  are/VBP\n",
      "  too/RB\n",
      "  large/JJ\n",
      "  or/CC\n",
      "  complex/JJ\n",
      "  to/TO\n",
      "  be/VB\n",
      "  dealt/VBN\n",
      "  with/IN\n",
      "  by/IN\n",
      "  (NP traditional/JJ data-processing/JJ application/NN)\n",
      "  (NP software/NN))\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "wiki = wikipedia.page('Big data').content.split(\".\")[0]\n",
    "tokens = word_tokenize(wiki)\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(tagged_tokens)\n",
    "print(result)\n",
    "#result.draw() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f926fa4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
